{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOd2I3y1BvzRO7VYyRVkMXZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zbovaird/OSINT/blob/main/Copy_of_OSINT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laUJ-dJdyZZU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OSINT Tools\n",
        "\n",
        "A categorized list of powerful, free (or freemium) OSINT tools with brief\n",
        "notes on programmatic API connectivity and whether they offer an MCP-server\n",
        "implementation (Model Context Protocol). \"API\" indicates a documented\n",
        "programmatic interface (REST/GraphQL/official SDK). \"MCP server\" is listed\n",
        "only if the project natively implements MCP semantics (rare).\n",
        "\n",
        "## Intelligence Platforms & Frameworks\n",
        "- MISP: API: Yes (comprehensive REST API). MCP server: No.\n",
        "- OpenCTI: API: Yes (REST + GraphQL). MCP server: No.\n",
        "- SpiderFoot (Community): API: Yes (web/server mode exposes HTTP endpoints). MCP server: No.\n",
        "- Maltego CE (Community Edition): API: Limited (transforms/TRX SDK; server features commercial). MCP server: No.\n",
        "- Recon-ng: API: No (module framework; scriptable/automatable via Python).\n",
        "\n",
        "## Internet-Wide Search / Scanners\n",
        "- Shodan: API: Yes (free limited tier with API key). MCP server: No.\n",
        "- Censys: API: Yes (free limited tier). MCP server: No.\n",
        "- ZoomEye: API: Yes (limited free access). MCP server: No.\n",
        "- BinaryEdge: API: Yes (free tester tier; mostly paid). MCP server: No.\n",
        "\n",
        "## Domain / DNS / WHOIS / Passive DNS\n",
        "- amass: API: No (CLI/Go library; programmatic usage via library). MCP server: No.\n",
        "- PassiveTotal / RiskIQ: API: Yes (primarily paid; limited free trials). MCP server: No.\n",
        "- Farsight DNSDB: API: Yes (commercial; limited community access). MCP server: No.\n",
        "- SecurityTrails: API: Yes (free tier/limited). MCP server: No.\n",
        "- `whois` (command-line): API: No (many registrars provide APIs separately).\n",
        "\n",
        "## Subdomain & Asset Discovery\n",
        "- Subfinder: API: No (CLI; libraries/wrappers exist). MCP server: No.\n",
        "- Sublist3r: API: No (CLI). MCP server: No.\n",
        "- Assetfinder: API: No (CLI). MCP server: No.\n",
        "- Amass (again): API: No (see above; widely used for passive/active discovery).\n",
        "- theHarvester: API: No (CLI; scriptable). MCP server: No.\n",
        "\n",
        "## Breach / Credentials / Dark Web\n",
        "- Have I Been Pwned (HIBP): API: Yes (API key; free for basic queries). MCP server: No.\n",
        "- DeHashed: API: Yes (limited/paid tiers). MCP server: No.\n",
        "- Ahmia (Tor search): API: Limited/No (public web index; some indexers provide APIs). MCP server: No.\n",
        "- OnionScan: API: No (tooling for Tor/hidden-service analysis; scriptable).\n",
        "\n",
        "## Social Media & People Search\n",
        "- snscrape: API: No (library/scraper; programmatic usage). MCP server: No.\n",
        "- Social media official APIs (X/Twitter, Facebook/Graph, Instagram, TikTok): API: Yes (access and free tiers vary, often require keys). MCP server: No.\n",
        "- Twint (historic): API: No (scraper; maintenance varies). MCP server: No.\n",
        "\n",
        "## Image, Metadata & Media Analysis\n",
        "- ExifTool: API: No (CLI/library). MCP server: No.\n",
        "- SauceNAO: API: Yes (free limited API keys). MCP server: No.\n",
        "- TinEye: API: Yes (commercial; limited free trial). MCP server: No.\n",
        "- FotoForensics / ELA tools: API: Limited/No.\n",
        "\n",
        "## Mapping & Geolocation\n",
        "- OpenStreetMap / Overpass API: API: Yes (free). MCP server: No.\n",
        "- Mapillary: API: Yes (community/free tiers). MCP server: No.\n",
        "- OpenAerialMap: API: Yes (varies). MCP server: No.\n",
        "\n",
        "## Network Recon & Scanning (OSINT-adjacent)\n",
        "- Nmap: API: No (CLI; libraries/wrappers exist). MCP server: No.\n",
        "- masscan: API: No (CLI). MCP server: No.\n",
        "- ZMap: API: No (CLI). MCP server: No.\n",
        "\n",
        "## Aggregation, Automation & Recon Suites\n",
        "- SpiderFoot HX (commercial cloud) / SpiderFoot Community: API: Yes (community/server exposes API). MCP server: No.\n",
        "- TheHarvester: API: No (see above). MCP server: No.\n",
        "- OSINT Framework (website index): API: No (catalog resource).\n",
        "\n",
        "## Browser Extensions & Utilities\n",
        "- Hunchly (paid): API: Limited (commercial). MCP server: No.\n",
        "- Hunter.io: API: Yes (free limited tier). MCP server: No.\n",
        "\n",
        "## Notes & Summary\n",
        "- API availability: Many powerful OSINT projects expose programmatic APIs (Shodan, Censys, VirusTotal, MISP, OpenCTI, Overpass, etc.), but free access is typically limited by quotas or requires registration and an API key.\n",
        "- MCP server: I did not find mainstream OSINT tools that natively implement the Model Context Protocol (MCP). Most are self-hostable services or CLI tools exposing REST/GraphQL endpoints you can integrate into automation or an MCP-compatible wrapper.\n",
        "\n",
        "If you want, I can:\n",
        "- export this list to Markdown or CSV, or\n",
        "- add one-line usage examples, install/run instructions, and direct links for each tool.\n",
        "\n",
        "Generated: 2026-02-16\n",
        "\n",
        "## Integration Index (MCP servers / API)\n",
        "This quick index groups the tools above by whether they provide an MCP-server, a programmatic API, or both.\n",
        "\n",
        "- Tools with MCP servers:\n",
        "\t- None (no mainstream OSINT projects in this list natively implement MCP semantics). Many can be wrapped or proxied behind an MCP server if desired.\n",
        "\n",
        "- Tools with API connections (documented programmatic interfaces / REST/GraphQL/official SDKs):\n",
        "\t- MISP\n",
        "\t- OpenCTI\n",
        "\t- SpiderFoot (Community / HX)\n",
        "\t- Maltego CE (limited transform/TRX integration)\n",
        "\t- Shodan\n",
        "\t- Censys\n",
        "\t- ZoomEye\n",
        "\t- BinaryEdge\n",
        "\t- PassiveTotal / RiskIQ\n",
        "\t- Farsight DNSDB\n",
        "\t- SecurityTrails\n",
        "\t- Have I Been Pwned (HIBP)\n",
        "\t- DeHashed\n",
        "\t- Social media official APIs (X/Twitter, Facebook/Graph, Instagram, TikTok)\n",
        "\t- SauceNAO\n",
        "\t- TinEye\n",
        "\t- OpenStreetMap / Overpass API\n",
        "\t- Mapillary\n",
        "\t- OpenAerialMap\n",
        "\t- SpiderFoot HX\n",
        "\t- Hunter.io\n",
        "\n",
        "\t## Political donations / Campaign finance (US)\n",
        "\tThis section lists public-data and OSINT-focused tools and APIs useful for tracing political donations, PACs, and campaign finance in the United States.\n",
        "\n",
        "\t- Federal Election Commission (FEC) / OpenFEC: API: Yes (official FEC REST API / OpenFEC endpoints with campaign finance filings, receipts, disbursements). MCP server: No.\n",
        "\t- OpenSecrets (Center for Responsive Politics): API: Yes (OpenSecrets API; requires API key, free limited access). MCP server: No.\n",
        "\t- FollowTheMoney (National Institute on Money in Politics / FollowTheMoney.org): API: Yes (data access and API/bulk downloads; registration may be required). MCP server: No.\n",
        "\t- LittleSis: API: Yes (API for persons/organizations/relationships; useful to correlate donors and recipients). MCP server: No.\n",
        "\t- MapLight: API: Limited (historically provided programmatic access; availability varies). MCP server: No.\n",
        "\t- Data.gov / Campaign finance datasets: API: Yes (various datasets and endpoints aggregated by Data.gov). MCP server: No.\n",
        "\t- ProPublica / Investigative datasets: API: Limited (ProPublica offers APIs for several datasets; campaign finance analysis often integrates FEC data). MCP server: No.\n",
        "\n",
        "\tNotes: The FEC and OpenSecrets APIs are the primary programmatic sources for raw campaign finance data. Many investigative datasets combine these sources with corporate registries and donor-lookup services. None of the above natively implement MCP semantics, but all with APIs can be wrapped behind an MCP server or ingested into an MCP-capable pipeline.\n",
        "\n",
        "\t## Additional OSINT Categories\n",
        "\tBelow are additional useful OSINT categories that were not previously listed, with representative tools and notes about API availability.\n",
        "\n",
        "\t- Public Records & Government Data\n",
        "\t\t- Examples: PACER / RECAP (court records), Data.gov datasets, state business registries, local property records. API: Varies (Data.gov: Yes; PACER: paid; RECAP: limited). MCP server: No.\n",
        "\n",
        "\t- Financial Filings & Corporate Intelligence\n",
        "\t\t- Examples: SEC EDGAR (API/bulk), OpenCorporates (API), Companies House (UK API). API: Yes (EDGAR bulk, OpenCorporates API). MCP server: No.\n",
        "\n",
        "\t- Academic & Publication Search\n",
        "\t\t- Examples: Semantic Scholar, CrossRef, Google Scholar (no official public API), PubMed. API: Semantic Scholar/CrossRef/PubMed: Yes (limited); Google Scholar: No official API. MCP server: No.\n",
        "\n",
        "\t- Code Repositories & Package Search\n",
        "\t\t- Examples: GitHub, GitLab, npm, PyPI, public repo search. API: Yes (GitHub/GitLab APIs; registry APIs). MCP server: No.\n",
        "\n",
        "\t- Mobile App Stores & APK Analysis\n",
        "\t\t- Examples: APKMirror, Google Play scraping tools, Mobile App security scanners. API: Limited (most stores restrict programmatic scraping; some services provide APIs). MCP server: No.\n",
        "\n",
        "\t- Phone Number, SMS & VOIP Lookup\n",
        "\t\t- Examples: Numverify, Twilio lookup, OpenCNAM, carrier lookup services. API: Yes (typically paid/free-tier). MCP server: No.\n",
        "\n",
        "\t- FOIA, Court & Legal Datasets\n",
        "\t\t- Examples: RECAP (court dockets), state court portals, FOIA request aggregators. API: Limited/varies. MCP server: No.\n",
        "\n",
        "\t- Satellite & High-Resolution Imagery\n",
        "\t\t- Examples: Sentinel Hub, USGS EarthExplorer, Planet (commercial), Google Earth Engine. API: Yes (Sentinel/USGS/GEE APIs; Planet commercial). MCP server: No.\n",
        "\n",
        "\t- Business & People Data Aggregators\n",
        "\t\t- Examples: Pipl, Clearbit, FullContact, Whitepages Pro. API: Yes (mostly commercial with limited free tiers). MCP server: No.\n",
        "\n",
        "\t- Forum / Paste / Leak Search\n",
        "\t\t- Examples: Pastebin scrapers, LeakForums trackers, public paste monitoring services. API: Varies (Pastebin: limited API). MCP server: No.\n",
        "\n",
        "\t- Darknet / Tor Monitoring Tools\n",
        "\t\t- Examples: OnionScan, Tor i2p indices, specialized darknet crawlers. API: Limited/varies. MCP server: No.\n",
        "\n",
        "\t- Document / PDF Analysis & Metadata Extraction\n",
        "\t\t- Examples: ExifTool (metadata), pdfgrep, Tika. API: Limited (libraries exist). MCP server: No.\n",
        "\n",
        "\tIf you'd like, I can fold these into the main list with brief links and one-line usage hints for each tool, or export the entire `tools` file to Markdown/CSV for sharing.\n",
        "\n",
        "\t## One-line Descriptions (what each tool is geared toward)\n",
        "\t- MISP: sharing, storing, and collaborating on threat intelligence (IOCs, events, attributes).\n",
        "\t- OpenCTI: building and querying threat intelligence knowledge graphs (actors, indicators, relationships).\n",
        "\t- SpiderFoot (Community / HX): automated reconnaissance across domains, IPs, emails, and OSINT sources.\n",
        "\t- Maltego CE: visual link analysis and relationship mapping between entities (people, domains, infrastructure).\n",
        "\t- Recon-ng: modular reconnaissance framework for automated data collection and enrichment.\n",
        "\t- Shodan: discovery and metadata on internet-connected devices, exposed services, and banners.\n",
        "\t- Censys: internet-wide host and certificate discovery with searchable metadata and fingerprints.\n",
        "\t- ZoomEye: searchable index of internet-facing assets and services (IPs, banners, hosts).\n",
        "\t- BinaryEdge: internet asset scanning and threat intelligence focused on exposed services and vulnerabilities.\n",
        "\t- amass: passive and active subdomain enumeration and attack-surface mapping.\n",
        "\t- PassiveTotal / RiskIQ: domain and passive DNS intelligence, historical records, and asset context.\n",
        "\t- Farsight DNSDB: historical passive DNS resolution data for domain/IP correlation.\n",
        "\t- SecurityTrails: domain history, DNS records, WHOIS, and hosting/ownership metadata.\n",
        "\t- whois (CLI/registrar APIs): domain registration and registrar/owner metadata.\n",
        "\t- Subfinder: fast passive subdomain discovery using many public sources.\n",
        "\t- Sublist3r: subdomain enumeration via search engines and public sources.\n",
        "\t- Assetfinder: finding related domains and assets for an organization or domain.\n",
        "\t- theHarvester: harvesting emails, subdomains, hosts, and related data from public sources.\n",
        "\t- Have I Been Pwned (HIBP): checking whether emails or domains appear in public data breaches.\n",
        "\t- DeHashed: searchable breach and credential datasets for exposed accounts and leaks.\n",
        "\t- Ahmia: search index for Tor hidden services and .onion sites.\n",
        "\t- OnionScan: discovery and analysis of Tor hidden services and metadata leakage.\n",
        "\t- snscrape: scraping public social media posts and profile metadata without official APIs.\n",
        "\t- Social media official APIs (X/Twitter, Facebook/Graph, Instagram, TikTok): programmatic access to posts, users, and engagement metadata.\n",
        "\t- Twint: Twitter scraping for timelines, searches, and user metadata (community-maintenance varies).\n",
        "\t- ExifTool: extracting embedded metadata from images, documents, and media files (EXIF, XMP).\n",
        "\t- SauceNAO: reverse-image search for image sources and visual matches.\n",
        "\t- TinEye: reverse-image search and matching to find image usage and origins.\n",
        "\t- FotoForensics / ELA tools: basic image integrity and Error Level Analysis to detect manipulation.\n",
        "\t- OpenStreetMap / Overpass API: querying crowd-sourced map data and geospatial features.\n",
        "\t- Mapillary: street-level imagery and location metadata from user-contributed photos.\n",
        "\t- OpenAerialMap: aggregated aerial and drone imagery for geolocation and analysis.\n",
        "\t- Nmap: port and service scanning, OS detection, and network reconnaissance.\n",
        "\t- masscan: very high-speed port scanner for scanning large IP ranges quickly.\n",
        "\t- ZMap: internet-scale network scanning framework for wide-area scans.\n",
        "\t- SpiderFoot HX (commercial) / SpiderFoot Community: centralized automated OSINT aggregation and reporting.\n",
        "\t- OSINT Framework (website): curated index of OSINT tools and categorized resources.\n",
        "\t- Hunchly: capturing and preserving web evidence with contextual notes and indexing.\n",
        "\t- Hunter.io: discovering and verifying professional email addresses associated with domains.\n",
        "\t- FEC / OpenFEC: official campaign finance filings, contributions, expenditures, and committee data.\n",
        "\t- OpenSecrets: lobbying, donation, and influence data linking donors, PACs, and candidates.\n",
        "\t- FollowTheMoney: state-level campaign finance data and donor-tracking across jurisdictions.\n",
        "\t- LittleSis: relationship mapping between people, organizations, and funding/influence networks.\n",
        "\t- MapLight: analysis of money-in-politics and policy influence (data access varies).\n",
        "\t- Data.gov (campaign datasets): federated government datasets including mapped campaign finance files.\n",
        "\t- ProPublica: investigative datasets and reporting often derived from public campaign finance sources.\n",
        "\t- PACER / RECAP: court dockets, filings, and public legal records (PACER is paid; RECAP archives some public data).\n",
        "\t- SEC EDGAR: corporate filings, financial disclosures, and officer/director relationships.\n",
        "\t- OpenCorporates: global company registry data and corporate metadata.\n",
        "\t- Companies House (UK): official company registry data and filings for UK entities.\n",
        "\t- Semantic Scholar / CrossRef / PubMed: academic paper metadata, citations, and publication records.\n",
        "\t- GitHub / GitLab / npm / PyPI: code repository and package metadata, commits, contributors, and dependency data.\n",
        "\t- APKMirror / Google Play scraping tools: mobile app metadata, APK artifacts, and historical versions (APIs limited).\n",
        "\t- Numverify / Twilio Lookup / OpenCNAM: phone-number validation, carrier lookup, and basic owner metadata.\n",
        "\t- RECAP / FOIA portals: aggregated court records and public-record FOIA-requested documents.\n",
        "\t- Sentinel Hub / USGS EarthExplorer / Planet / Google Earth Engine: satellite & aerial imagery datasets and geospatial analysis APIs.\n",
        "\t- Pipl / Clearbit / FullContact / Whitepages Pro: person and company enrichment data (commercial aggregation).\n",
        "\t- Pastebin scrapers / leak trackers: monitoring paste sites and public leak postings for exposed data.\n",
        "\t- LeakForums trackers / darknet monitors: indexes and alerts for content posted on darknet forums.\n",
        "\t- Tika / pdfgrep / document parsers: extracting text and metadata from PDFs and documents for content analysis.\n",
        "\n",
        "\t## Real-time monitoring (events, incidents, and live signals)\n",
        "\tThis section lists tools and data sources suitable for tracking domestic events and incidents in near real time.\n",
        "\n",
        "\t- Social listening / micro-post streams:\n",
        "\t\t- X/Twitter API: real-time tweets, search and filtered streams (requires API key and rate limits).\n",
        "\t\t- CrowdTangle (Meta): public page/group monitoring (research access; near real-time).\n",
        "\t\t- snscrape: programmatic scraping of public social posts (not an official API).\n",
        "\n",
        "\t- Messaging & community channels:\n",
        "\t\t- Telegram API / Telethon: monitor public channels and posts programmatically.\n",
        "\t\t- Reddit API / Pushshift: monitor live subreddit activity and threads.\n",
        "\t\t- Discord bots/APIs: monitor public servers where permitted.\n",
        "\n",
        "\t- Live maps & incident aggregators:\n",
        "\t\t- LiveUAMap and similar crowd-sourced incident maps: visual tracking of civil unrest and conflict.\n",
        "\t\t- Local community-run incident maps and feeds (varies by region).\n",
        "\n",
        "\t- First-responder & radio feeds:\n",
        "\t\t- Broadcastify: live police/fire/EMS radio streams in many jurisdictions.\n",
        "\t\t- Local scanner / 911 audio streams and feeds (availability varies; check local terms).\n",
        "\n",
        "\t- News, alerts & media monitoring:\n",
        "\t\t- Google News / GDELT / MediaCloud: near real-time media ingestion and analytics (APIs available).\n",
        "\t\t- RSS feeds and local news live-blogs for immediate situation updates.\n",
        "\n",
        "\t- Traffic, transit & camera feeds:\n",
        "\t\t- State DOT traffic cameras and incident feeds (many provide public camera URLs and APIs).\n",
        "\t\t- Waze for Cities / INRIX: traffic incident streams (partnership/commercial access).\n",
        "\n",
        "\t- Aviation & maritime telemetry:\n",
        "\t\t- ADS-B / Flight tracking (FlightRadar24, ADSBExchange): live flight positions and metadata.\n",
        "\t\t- AIS / MarineTraffic: vessel movements and port activity streams.\n",
        "\n",
        "\t- Emergency / hazard sensors:\n",
        "\t\t- USGS earthquake feeds (real-time API).\n",
        "\t\t- NWS / NOAA alerts and weather watches (APIs and RSS feeds).\n",
        "\t\t- NASA FIRMS / VIIRS: near-real-time thermal hotspot (fire) detections.\n",
        "\n",
        "\t- Satellite / imagery (frequent updates):\n",
        "\t\t- Sentinel Hub / USGS / Planet / Google Earth Engine: repeated imagery and analytics (APIs; some commercial).\n",
        "\n",
        "\t- Public safety & government feeds:\n",
        "\t\t- Local government incident dashboards and open data portals (varies by city/county).\n",
        "\t\t- Data.gov aggregated datasets and emergency feeds.\n",
        "\n",
        "\t- Specialized monitoring:\n",
        "\t\t- Pastebin/Dump monitors and darknet monitoring services: watch for leaked data or operational chatter (legal precautions required).\n",
        "\n",
        "\tOperational notes:\n",
        "\t- Trade-offs: social feeds are fastest but noisy; official feeds are authoritative but slower. Combine both for signal/verification.\n",
        "\t- Legal/ethical: respect platform ToS, PII rules, and local laws. Prefer official APIs and rate-limit handling; consider on-prem LLMs for sensitive analysis.\n",
        "\n",
        "\t## Real-time monitoring — links & quick setup notes\n",
        "\t- X / Twitter API: https://developer.twitter.com/en/docs/twitter-api\n",
        "\t\t- Quick setup: register developer account, create project/app, get bearer/token keys; use filtered stream endpoints for live tweets; implement rate‑limit handling and replay storage.\n",
        "\n",
        "\t- CrowdTangle (Meta): https://www.crowdtangle.com/\n",
        "\t\t- Quick setup: request researcher access or apply for API access; use to track public Pages, Groups, and Instagram accounts with near-real-time ingestion.\n",
        "\n",
        "\t- snscrape: https://github.com/JustAnotherArchivist/snscrape\n",
        "\t\t- Quick setup: Python package install `pip install snscrape`; use cron or streaming loops to collect recent posts; respect ToS and scraping limits.\n",
        "\n",
        "\t- Telegram API / Telethon: https://core.telegram.org/api, https://docs.telethon.dev/\n",
        "\t\t- Quick setup: create a Telegram app to get API id/hash, use Telethon or pyrogram to subscribe to public channels, and persist messages via webhooks or DB.\n",
        "\n",
        "\t- Reddit API / Pushshift: https://www.reddit.com/dev/api/, https://pushshift.io/\n",
        "\t\t- Quick setup: register Reddit app for OAuth credentials; use Reddit streaming libraries or Pushshift for historical/near-real-time data (Pushshift has rate/availability caveats).\n",
        "\n",
        "\t- Discord (bots / API): https://discord.com/developers/docs/intro\n",
        "\t\t- Quick setup: create a bot, add to servers with permission, use gateway events or webhooks to capture messages (respect server rules and privacy).\n",
        "\n",
        "\t- LiveUAMap: https://liveuamap.com/\n",
        "\t\t- Quick setup: use the site for visual monitoring; programmatic access limited — consider scraping feeds carefully and respecting terms.\n",
        "\n",
        "\t- Broadcastify (scanner audio): https://www.broadcastify.com/\n",
        "\t\t- Quick setup: browse public streams and embed or capture via provided stream URLs; check local legality and terms before recording.\n",
        "\n",
        "\t- GDELT (media monitoring): https://blog.gdeltproject.org/gdelt-2-0-our-global-world-in-realtime/\n",
        "\t\t- Quick setup: use GDELT event and media APIs for near real-time media mentions and tone analysis; ingest via scheduled pulls.\n",
        "\n",
        "\t- MediaCloud: https://mediacloud.org/\n",
        "\t\t- Quick setup: request API key for research; query stories and social amplification metrics programmatically.\n",
        "\n",
        "\t- State DOT traffic cameras / feeds: (varies by state) — common pattern: DOT websites list camera URLs and incident feeds\n",
        "\t\t- Quick setup: identify your state/city DOT portal; many offer JSON feeds or camera endpoints you can poll or proxy.\n",
        "\n",
        "\t- Waze for Cities / INRIX: https://www.waze.com/ccp, https://inrix.com/\n",
        "\t\t- Quick setup: apply for partnership or data access; commercial access gives streaming of incidents and jam data.\n",
        "\n",
        "\t- ADS‑B / Flight tracking (ADSBExchange): https://www.adsbexchange.com/\n",
        "\t\t- Quick setup: use community feeds or APIs to stream flight positions; FlightRadar24 offers commercial APIs.\n",
        "\n",
        "\t- MarineTraffic / AIS: https://www.marinetraffic.com/\n",
        "\t\t- Quick setup: commercial APIs available for vessel positions and port calls; community AIS feeders provide raw streams.\n",
        "\n",
        "\t- USGS earthquake feeds: https://earthquake.usgs.gov/fdsnws/event/1/\n",
        "\t\t- Quick setup: use the FDSN event API or GeoJSON feeds for near-real-time earthquake notifications.\n",
        "\n",
        "\t- NWS / NOAA alerts API: https://www.weather.gov/documentation/services-web-api\n",
        "\t\t- Quick setup: poll alerts or subscribe to CAP feeds for watches/warnings; integrate geofencing for affected areas.\n",
        "\n",
        "\t- NASA FIRMS / VIIRS thermal hotspots: https://firms.modaps.eosdis.nasa.gov/\n",
        "\t\t- Quick setup: download near-real-time CSV/GeoJSON or use APIs to ingest hotspot detections (fires, thermal anomalies).\n",
        "\n",
        "\t- Sentinel Hub / USGS / Planet imagery: https://www.sentinel-hub.com/, https://earthexplorer.usgs.gov/, https://www.planet.com/\n",
        "\t\t- Quick setup: register for API keys (Sentinel/USGS free tiers available); schedule frequent tile pulls or subscribe to change-detection services.\n",
        "\n",
        "\t- Data.gov emergency & city datasets: https://www.data.gov/\n",
        "\t\t- Quick setup: search for local incident datasets and subscribe to publisher endpoints or RSS; ingest via ETL jobs.\n",
        "\n",
        "\t- Pastebin API: https://pastebin.com/api\n",
        "\t\t- Quick setup: register for API key; poll new pastes or use monitored lists; treat leaked data carefully and follow legal constraints.\n",
        "\n",
        "\t- Darknet / Tor monitoring tools (OnionScan, darknet monitors): https://github.com/s-rah/onionscan\n",
        "\t\t- Quick setup: run crawlers responsibly, store metadata (not raw illegal content), and rely on commercial darknet monitoring providers for broad coverage.\n",
        "\n",
        "\tIf you want, I can (a) add these links into `tools` as clickable markdown-style links, (b) create short example scripts (Python) to subscribe to one or two of these feeds, or (c) export a CSV of these sources and setup notes. Tell me which option you prefer.\n",
        "\n",
        "- Tools with both MCP server + API:\n",
        "\t- None identified (APIs are common; native MCP implementations were not found).\n",
        "\n",
        "Notes: \"API connections\" above includes tools marked as \"Yes\" or \"Limited/Yes\" in the main list. If you want, I can produce a CSV/Markdown table with exact API endpoints, sign-up links, and notes about free-tier limits.\n"
      ],
      "metadata": {
        "id": "WypxfyX8yhKv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Telegram channel listener example (Telethon)\n",
        "\n",
        "Requirements:\n",
        "- pip install telethon\n",
        "\n",
        "Usage:\n",
        "- Obtain `api_id` and `api_hash` from https://my.telegram.org\n",
        "- Set `CHANNEL` to the public channel username or ID to monitor.\n",
        "- Optional: set `WEBHOOK_URL` to forward collected messages as POST JSON.\n",
        "\n",
        "This script listens for new messages in a channel and appends them to a local JSONL file.\n",
        "\"\"\"\n",
        "import asyncio\n",
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "from telethon import TelegramClient, events\n",
        "\n",
        "# === Configure ===\n",
        "API_ID = int(os.getenv('TG_API_ID', '0'))  # replace or set env TG_API_ID\n",
        "API_HASH = os.getenv('TG_API_HASH', '')   # replace or set env TG_API_HASH\n",
        "CHANNEL = os.getenv('TG_CHANNEL', 'examplechannel')  # e.g. 'cnn'\n",
        "OUTPUT_FILE = os.getenv('TG_OUTPUT', 'telegram_messages.jsonl')\n",
        "WEBHOOK_URL = os.getenv('TG_WEBHOOK', '')  # optional: POST new messages\n",
        "PERSIST_EVERY = 1\n",
        "# =================\n",
        "\n",
        "if API_ID == 0 or API_HASH == '':\n",
        "    raise SystemExit('Set API_ID and API_HASH (see script header).')\n",
        "\n",
        "client = TelegramClient('osint_session', API_ID, API_HASH)\n",
        "\n",
        "@client.on(events.NewMessage(chats=CHANNEL))\n",
        "async def handler(event):\n",
        "    msg = event.message\n",
        "    data = {\n",
        "        'id': msg.id,\n",
        "        'date': msg.date.isoformat() if hasattr(msg, 'date') else None,\n",
        "        'sender_id': getattr(msg.sender, 'id', None) if msg.sender else None,\n",
        "        'text': msg.message,\n",
        "        'raw': str(msg.to_dict()),\n",
        "    }\n",
        "\n",
        "    # append to JSONL file\n",
        "    with open(OUTPUT_FILE, 'a', encoding='utf-8') as fh:\n",
        "        fh.write(json.dumps(data, ensure_ascii=False) + '\\n')\n",
        "\n",
        "    print(f\"Saved message {data['id']} from {CHANNEL} at {data['date']}\")\n",
        "\n",
        "    if WEBHOOK_URL:\n",
        "        # optional: send to webhook (best-effort, don't block)\n",
        "        import requests\n",
        "        try:\n",
        "            requests.post(WEBHOOK_URL, json=data, timeout=5)\n",
        "        except Exception as e:\n",
        "            print('Webhook POST failed:', e)\n",
        "\n",
        "async def main():\n",
        "    await client.start()\n",
        "    print(f'Listening to {CHANNEL} -- output -> {OUTPUT_FILE}')\n",
        "    await client.run_until_disconnected()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    try:\n",
        "        asyncio.run(main())\n",
        "    except KeyboardInterrupt:\n",
        "        print('Stopped by user')\n"
      ],
      "metadata": {
        "id": "wx4w0M3Aymjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "USGS alerts poller example\n",
        "\n",
        "Requirements:\n",
        "- pip install requests\n",
        "\n",
        "Usage:\n",
        "- This script polls the USGS \"all_hour\" GeoJSON feed and prints/saves new events.\n",
        "- Configure `POLL_INTERVAL` for how often to poll (seconds).\n",
        "- Optionally set `WEBHOOK_URL` to POST new events.\n",
        "\"\"\"\n",
        "import json\n",
        "import time\n",
        "import requests\n",
        "import os\n",
        "\n",
        "FEED_URL = os.getenv('USGS_FEED', 'https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/all_hour.geojson')\n",
        "POLL_INTERVAL = int(os.getenv('USGS_POLL_INTERVAL', '60'))\n",
        "OUTPUT_FILE = os.getenv('USGS_OUTPUT', 'usgs_new_events.jsonl')\n",
        "WEBHOOK_URL = os.getenv('USGS_WEBHOOK', '')\n",
        "SEEN_FILE = os.getenv('USGS_SEEN', '.usgs_seen.json')\n",
        "\n",
        "seen = set()\n",
        "if os.path.exists(SEEN_FILE):\n",
        "    try:\n",
        "        with open(SEEN_FILE, 'r', encoding='utf-8') as fh:\n",
        "            seen = set(json.load(fh))\n",
        "    except Exception:\n",
        "        seen = set()\n",
        "\n",
        "print('Polling USGS feed:', FEED_URL)\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        r = requests.get(FEED_URL, timeout=20)\n",
        "        r.raise_for_status()\n",
        "        feed = r.json()\n",
        "        features = feed.get('features', [])\n",
        "\n",
        "        new_events = []\n",
        "        for f in features:\n",
        "            eid = f.get('id')\n",
        "            if not eid or eid in seen:\n",
        "                continue\n",
        "            seen.add(eid)\n",
        "            props = f.get('properties', {})\n",
        "            geom = f.get('geometry', {})\n",
        "            evt = {\n",
        "                'id': eid,\n",
        "                'time': props.get('time'),\n",
        "                'place': props.get('place'),\n",
        "                'mag': props.get('mag'),\n",
        "                'url': props.get('url'),\n",
        "                'geometry': geom,\n",
        "                'properties': props,\n",
        "            }\n",
        "            new_events.append(evt)\n",
        "\n",
        "            # append to file\n",
        "            with open(OUTPUT_FILE, 'a', encoding='utf-8') as fh:\n",
        "                fh.write(json.dumps(evt) + '\\n')\n",
        "\n",
        "            print('New event:', eid, evt['place'], 'mag', evt['mag'])\n",
        "\n",
        "            if WEBHOOK_URL:\n",
        "                try:\n",
        "                    requests.post(WEBHOOK_URL, json=evt, timeout=5)\n",
        "                except Exception as e:\n",
        "                    print('Webhook post failed:', e)\n",
        "\n",
        "        # persist seen ids\n",
        "        with open(SEEN_FILE, 'w', encoding='utf-8') as fh:\n",
        "            json.dump(list(seen), fh)\n",
        "\n",
        "    except Exception as e:\n",
        "        print('Error polling USGS feed:', e)\n",
        "\n",
        "    time.sleep(POLL_INTERVAL)\n"
      ],
      "metadata": {
        "id": "x-JWdzfyyot9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}